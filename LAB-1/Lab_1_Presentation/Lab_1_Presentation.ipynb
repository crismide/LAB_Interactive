{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v3 as iio\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from camera_widget import cv2_snap, cv2_vid\n",
    "from IPython.display import Image, Video\n",
    "from feat import Detector\n",
    "import opencv_jupyter_ui as jcv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Manipulation\n",
    "\n",
    "We can load images as `ndarray`s shaped `(height, width, channels)` using `iio.imread()`, and display them with `plt.imshow()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seagull = iio.imread(\"seagull.jpg\") # Copyright Yuqiong Wang, 2023. Used with permission from the author.\n",
    "plt.imshow(seagull)\n",
    "type(seagull), seagull.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, the type is `uint8`: integers in range 0 ... 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seagull.dtype, seagull.min(), seagull.mean(), seagull.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the picture is represented as an array, we can easily manipulate it. E.g., we can use *index slicing* to crop it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutout = seagull[190:640, 525:975]\n",
    "plt.imshow(cutout)\n",
    "cutout.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we do an extreme crop, we can see the individual pixels that form the image. In RGB, each pixel is represented by 3 values: Red, Green, and Blue intensities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_cutout = seagull[230:255, 595:620]\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.imshow(extreme_cutout)\n",
    "extreme_cutout.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that our array stores the colors as RGB, let's set R to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_red = cutout.copy()\n",
    "no_red[:, :, 0] = 0\n",
    "plt.imshow(no_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can invert colors with some math:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse = 255 - cutout\n",
    "plt.imshow(inverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iio.imwrite()` allows us to save the image to a variety of formats. In this case, `JPG`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iio.imwrite(\"inverse.jpg\", inverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a PNG with transparency now. In this case, the channels are `RGBA`: Red, Green, Blue, Alpha.\n",
    "* `A = 0` means fully transparent\n",
    "* `A = 255` means fully opaque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uu_logo = iio.imread(\"uu_logo.png\") # Trademark owned by Uppsala University.\n",
    "plt.imshow(uu_logo)\n",
    "uu_logo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since PyPlot adds a white background, it's not so obvious that we have transparency. Let's make everything fully opaque to reveal the hidden RGB colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_alpha = uu_logo.copy()\n",
    "full_alpha[:, :, 3] = 255\n",
    "plt.imshow(full_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get tired of the PyPlot numbers, in a Jupyter notebook you can use `IPython.display.Image`. It even respects transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"uu_logo.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Manipulation\n",
    "\n",
    "There is no good way to display videos using PyPlot, but we can use `IPython.display.Video` to embed a video in a notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"cats.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same command `iio.imread()` we used for images can also be used for videos. In this case, the array is shaped `(frames, height, width, channels)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"cats\": copyright Marc Fraile, 2023.\n",
    "cats = iio.imread(\"cats.mp4\")\n",
    "cats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the video as a sequence of frames discards some important information, like the framerate. We can recover this information (and other *metadata*) using `iio.immeta()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats_meta = iio.immeta(\"cats.mp4\")\n",
    "cats_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can display individual frames just like we display images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cats[200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "In this first block, we will practice one of the oldest and simplest tricks in Image Analysis: **color thresholding**. We will try to isolate a brightly colored object in an image by checking color values.\n",
    "\n",
    "1. `exercise_1_toy_knife_rgb.py`: use color segmentation using RGB directly.\n",
    "2. `exercise_2_toy_knife_hsv.py`: let's try again with a different representation of color: HSV (Hue - Saturation - Value).\n",
    "\n",
    "### Hints\n",
    "\n",
    "* `np.zeros_like(image, shape=...)`\n",
    "* You can use comparisons to index into an `ndarray`.\n",
    "* You can copy most code from Exercise 1 into Exercise 2.\n",
    "* `cv2.cvtColor()`\n",
    "* `cv2` also has loading and saving functions `cv2.imread()` and `cv2.imwrite()`, but they expect BGR and don't work with videos.\n",
    "* Use the internet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webcam Access\n",
    "\n",
    "If you look up online how to do any image processing task in Python, you will be told to use [OpenCV](https://opencv.org/). This is an old C++ library with a clunky Python interface, and has plenty of downsides, but it's hard to beat it in number of features or speed of execution.\n",
    "\n",
    "We will use OpenCV for real-time webcam access. To smooth over its usage in notebooks, we will also use `opencv_jupyter_ui`.\n",
    "\n",
    "We have written an utility to take snapshots and another to take videos using OpenCV. You can find the code in the module `camera_widget`. Let's test them out, starting with the picture-taking app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snap = cv2_snap()\n",
    "snap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(snap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's check the video-taking solution next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps, vid = cv2_vid()\n",
    "fps, vid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(vid[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display the video, let's save it to a local file first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iio.imwrite(\"webcam_video.mp4\", vid, fps=fps)\n",
    "Video(\"webcam_video.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a simplified version of the code we used in `camera_widget` to show a live-feed of our webcam using OpenCV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # check = True means we managed to get a frame.\n",
    "    # If check = False, the device is not available, and we should quit.\n",
    "    check, frame = cam.read()\n",
    "    if not check:\n",
    "        break\n",
    "\n",
    "    # OpenCV uses a separate window to display output.\n",
    "    jcv2.imshow(\"video\", frame)\n",
    "\n",
    "    # Press ESC to exit.\n",
    "    key = jcv2.waitKey(1) & 0xFF\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "jcv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get creative and output the channels separately. Note that OpenCV does not follow the standard RGB convention, using BGR instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import opencv_jupyter_ui as jcv2\n",
    "import numpy as np\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    check, in_frame = cam.read()\n",
    "    if not check:\n",
    "        break\n",
    "\n",
    "    (h, w, c) = in_frame.shape\n",
    "\n",
    "    out_frame = np.zeros_like(in_frame, shape=(h*2, w*2, c))\n",
    "\n",
    "    out_frame[:h,:w,:] = in_frame\n",
    "    out_frame[:h,w:,0] = in_frame[:,:,0]\n",
    "    out_frame[h:,:w,1] = in_frame[:,:,1]\n",
    "    out_frame[h:,w:,2] = in_frame[:,:,2]\n",
    "\n",
    "    jcv2.imshow(\"video\", out_frame)\n",
    "\n",
    "    # Press ESC to exit.\n",
    "    key = jcv2.waitKey(10) & 0xFF\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "jcv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "In this course, we are interested in features related to the expression of emotion. We will focus in facial features: the position of the face, the activation of different muscles used to express emotion, or even the expressed emotion itself. [Py-Feat](https://py-feat.org/) is a modern Python library that allows us to easily work with all these feature types.\n",
    "\n",
    "Let's load a detector and test it on the faces of the TAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = Detector(device=\"cuda\")\n",
    "detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the detector packages several models with different functions: finding faces in a picture, detecting key points (landmarks) in each face, deducing facial muscle activations (AUs), detecting emotion...\n",
    "\n",
    "We can pass a filename to `detector.detect_image()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"lux.jpg\", width=480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lux_prediction = detector.detect_image(\"lux.jpg\")\n",
    "print(type(lux_prediction))\n",
    "lux_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see it returned a `Fex`, which is a subclass of a Pandas `DataFrame`. It contains one row per face detected, and a bumch of features related to the face. `Fex` has a few added helper functions, like `plot_detections()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lux_prediction.plot_detections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about Alessio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alessio_prediction = detector.detect_image(\"alessio.jpg\")\n",
    "alessio_prediction.plot_detections()\n",
    "display(Image(\"alessio.jpg\", width=480))\n",
    "alessio_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this case the detector found a \"false positive\" in the background, so it's returning the data for two \"faces\". Apparently, the building in the background was angry.\n",
    "\n",
    "Speaking of angry faces..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marc_prediction = detector.detect_image(\"marc.jpg\")\n",
    "marc_prediction.plot_detections()\n",
    "display(Image(\"marc.jpg\"))\n",
    "marc_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUs correspond to muscle activations in the face, and can be used to predict emotion. We can ask Py-Feat to display the detected AU activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marc_prediction.plot_detections(faces=\"aus\", muscles={\"all\": \"heatmap\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lux_prediction.plot_detections(faces=\"aus\", muscles={\"all\": \"heatmap\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "In this block, we will apply two **face tracking** approaches to a live feed from our webcam: using both old-school vs. modern solutions. In the modern case, we will add **emotion detection**.\n",
    "\n",
    "3. `exercise_3_face_tracking.py`: use OpenCV to run a classic algorithm in the CPU. It only detects faces.\n",
    "4. `exercise_4_emotion_detection.ipynb`: use Py-Feat to run a neural-net based algorithm in the GPU. It detects faces, and which emotion they express.\n",
    "\n",
    "### Hints\n",
    "\n",
    "* Exercise 3:\n",
    "    * You can copy a lot from previous exercises.\n",
    "    * How good is the \"classic\" solution?\n",
    "        * Does it run fast?\n",
    "        * How easy is it to make it lose track of your face?\n",
    "        * How easy is it to get false positives? (detect faces where there are none).\n",
    "* Exercise 4:\n",
    "    * You need to use a more involved part of the `Detector` API:\n",
    "        * `detector.detect_faces()`\n",
    "        * `detector.detect_landmarks()`\n",
    "        * `detector.detect_emotions()`\n",
    "        * What do these functions do?\n",
    "    * Another helpful function call: `cam.set(cv2.CAP_PROP_BUFFERSIZE, 1)`\n",
    "        * What does this function call do?\n",
    "    * How good is the \"modern\" solution?\n",
    "        * Does it run fast?\n",
    "        * How easy is it to make it lose track of your face?\n",
    "        * How easy is it to get false positives? (detect faces where there are none)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
